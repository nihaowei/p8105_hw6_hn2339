---
title: "p8105_hw6_hn2339"
author: "Haowei Ni"
date: "2018/11/18"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(dplyr)
library(stringr)
library(readr)
library(leaps)
library(modelr)
library(mgcv)
library(broom)
```

## Question 1

```{r}
library(readr)
homicide = read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names()
```

Create a city_state variable and a binary variable 

```{r}
homicide = 
  homicide %>% 
  mutate(city_state = str_c(city, state, sep = "_"),  
         resolved = as.numeric(disposition == "Closed by arrest"))
```

Omit city Dallas, TX; Phoenix, AZ; and Kansas City, MO and Tulsa, AL

```{r}
homicide = 
  homicide %>% 
  filter(!(city_state == "Dallas_TX" | city_state == "Phoenix_AZ" | city_state == "Kansas City_MO" | city_state == "Tulsa_AL")) %>% 
  mutate(victim_race = fct_relevel(ifelse(victim_race == "White", "white", "non-white"), "white"))
```

Modifiy victim_race, and make victim_age numeric

```{r}
homicide = 
  homicide %>%
  mutate(victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White"))
```

fit a logistic regression

```{r}
baltimore_df = 
  homicide %>% 
  filter(city == "Baltimore")
```

```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
```

obtain the estimate and confidence interval of the adjusted odds ratio

```{r}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate), # transform back
         lower_bound = exp(estimate - std.error*1.96),
         upper_bound = exp(estimate + std.error*1.96)) %>%
  select(term, log_OR = estimate, OR, lower_bound, upper_bound, p.value) %>% 
  knitr::kable(digits = 3)
```

the odds ratio estimate is 0.441 and 95% CI is (0.313, 0.620)

```{r}
homicide_all =
  homicide %>% 
  group_by(city_state) %>% 
  nest()
OR_plot =
  homicide_all %>% 
  mutate(models = map(data, ~glm(resolved ~ victim_age + victim_sex + victim_race, data = .x, family = binomial())),
    models = map(models, broom::tidy)) %>% 
  select(-data) %>% 
  unnest() %>% 
  filter(term == "victim_racenon-white") %>% 
  mutate(OR = exp(estimate),
         lower_bound = exp(estimate - std.error*1.96),
         upper_bound = exp(estimate + std.error*1.96)) 
```

```{r}
OR_plot %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) +
  theme_bw() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        axis.text.x = element_text(angle = 90, size = 8),
        legend.key.size = unit(0.05, "cm"))
```

From this plot, we can see that Boston_MA has the lowest OR means that the non-white people's case and the white people's case are the same level difficulty to resolve.Tampa_FL has the highest OR means that non-white people's cases are much in the same solving rate as the white people. There are no much difference. Some cities have large error bar means that the there are much influction in the rate of solving the cases. 

## Question 2 

load and clean data. 

```{r}
birthweight = read_csv("data/birthweight.csv") %>% 
  mutate(mrace = as.factor(mrace),
         frace = as.factor(frace),
         babysex = as.factor(babysex),
         malform = as.factor(malform))
```

In order to find the best fitted model, we need to compare the adjusted R^2. 
We can use the "lm.regsubsets" function. 

```{r}
model = regsubsets(bwt ~ ., data = birthweight)
summary(model)
```

The '*' means that correlation is significant, so we need to include this variable in the model. So for the best fitted model, we choose 'babysex', 'bhead', 'blength', 'delwt', 'frace4', 'gaweeks', 'mrace2', 'ppbmi', 'smoken'.

```{r}
fit_full = lm(bwt~., data = birthweight)
step(fit_full, direction = "backward")
```

Use the criterion-based procedures, we choose the model with the smallest AIC value. 

```{r}
best_fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
    mheight + mrace + parity + ppwt + smoken, data = birthweight)
summary(best_fit)
```

Make two other models 

```{r}
lin_model1 = lm(bwt ~ blength + gaweeks, data = birthweight)
summary(lin_model1)

lin_model2 = lm(bwt ~ bhead * blength * babysex, data = birthweight)
summary(lin_model2)
```

The multiple R^2 for the 'best_fit' linear regression is 0.7181, the multiple R^2 for the other two is 0.5769 and 0.6849, so the model we choose is better. 

```{r}
birthweight %>% 
  add_residuals(best_fit) %>% 
  add_predictions(best_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point() + ggtitle("residuals vs prediction value ")
```

Cross validation 

```{r}
cv_df = crossv_mc(birthweight, 100, test = 0.2) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
```

```{r}
cv_result <- 
  cv_df %>% 
  mutate(lin_best = map(train, ~lm(bwt~babysex + bhead + blength + gaweeks + delwt + mheight + mrace + parity + ppwt + smoken + fincome, data = birthweight)),
         lin_mod2 = map(train, ~lm(bwt ~ blength + gaweeks, data = birthweight)),
         lin_mod3 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = birthweight))) %>%  
  mutate(rmse_lin_best = map2_dbl(best_fit, test, ~rmse(model = best_fit, data = birthweight)),
         rmse_mod2 = map2_dbl(lin_model1, test, ~rmse(model = lin_model1, data = birthweight)),
         rmse_mod3 = map2_dbl(lin_model2, test, ~rmse(model = lin_model2, data = birthweight))) 
```

```{r}
cv_result %>% 
  dplyr::select(.id, starts_with("rmse")) %>% 
  gather(key = model, value = rmse, rmse_lin_best:rmse_mod3) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

